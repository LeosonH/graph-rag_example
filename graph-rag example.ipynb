{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f55fccd-3c30-4430-9173-026e6f48f5ac",
   "metadata": {},
   "source": [
    "## Implementing Nano Graph RAG\n",
    "\n",
    "Author: Leoson Hoay\n",
    "\n",
    "This notebook provides an example of using the [nano-graphrag](https://github.com/gusye1234/nano-graphrag) implementation to create a simple Graph RAG knowledge base of a python code repository, using ChatGPT as the base model. The repository used for this example is [lshhdc](https://github.com/LeosonH/lshhdc), which is an implementation of Locality Sensitive Hashing (using cryptographic hashing) for matching similar pairs of data.\n",
    "\n",
    "### Preparation:\n",
    "- Clone [lshhdc](https://github.com/LeosonH/lshhdc) locally.\n",
    "- Obtain an OpenAI API Key, and add it to the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb3c0d1-a74d-407e-8435-7d03e9c34bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %env OPENAI_API_KEY=sk-proj..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef00dbcd-6d0d-4ccd-885f-e4f6cca8a86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "import json\n",
    "from nano_graphrag import GraphRAG, QueryParam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d8f75c1-6810-48cd-8b0f-445a2ed193ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:nano-graphrag:Load KV full_docs with 0 data\n",
      "INFO:nano-graphrag:Load KV text_chunks with 0 data\n",
      "INFO:nano-graphrag:Load KV llm_response_cache with 0 data\n",
      "INFO:nano-graphrag:Load KV community_reports with 0 data\n",
      "INFO:nano-graphrag:Loaded graph from ./lshhdc-db\\graph_chunk_entity_relation.graphml with 0 nodes, 0 edges\n",
      "INFO:nano-vectordb:Load (0, 1536) data\n",
      "INFO:nano-vectordb:Init {'embedding_dim': 1536, 'metric': 'cosine', 'storage_file': './lshhdc-db\\\\vdb_entities.json'} 0 data\n"
     ]
    }
   ],
   "source": [
    "# Initialize GraphRAG instance\n",
    "graph_rag = GraphRAG(working_dir=\"./lshhdc-db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4154522-cf97-4b63-8787-85257b2ee724",
   "metadata": {},
   "outputs": [],
   "source": [
    "CODEBASE_DIR = \"./lshhdc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c51876e-7e65-4d6a-8914-67755b3646cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AST parsing error: Missing parentheses in call to 'print'. Did you mean print(...)? (<unknown>, line 302)\n",
      "AST parsing error: Missing parentheses in call to 'print'. Did you mean print(...)? (<unknown>, line 24)\n"
     ]
    }
   ],
   "source": [
    "# Extract docstrings via AST\n",
    "def extract_docstrings(file_content):\n",
    "    docstrings = []\n",
    "    try:\n",
    "        parsed_ast = ast.parse(file_content)\n",
    "        for node in ast.walk(parsed_ast):\n",
    "            if isinstance(node, (ast.FunctionDef, ast.ClassDef, ast.Module)):\n",
    "                doc = ast.get_docstring(node)\n",
    "                if doc:\n",
    "                    obj_type = type(node).__name__\n",
    "                    obj_name = getattr(node, 'name', 'Module')\n",
    "                    docstrings.append(f\"{obj_type} '{obj_name}': {doc}\")\n",
    "    except Exception as e:\n",
    "        print(f\"AST parsing error: {e}\")\n",
    "    return docstrings\n",
    "\n",
    "# Extract inline comments\n",
    "def extract_inline_comments(file_content):\n",
    "    comments = []\n",
    "    for line in file_content.split(\"\\n\"):\n",
    "        stripped = line.strip()\n",
    "        if stripped.startswith(\"#\"):\n",
    "            comments.append(stripped.lstrip(\"# \").strip())\n",
    "    return comments\n",
    "\n",
    "# Extract notebook comments (.ipynb)\n",
    "def extract_notebook_comments(filepath):\n",
    "    comments = []\n",
    "    try:\n",
    "        with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "            notebook = json.load(f)\n",
    "        for cell in notebook.get('cells', []):\n",
    "            if cell['cell_type'] == 'code':\n",
    "                for line in cell['source']:\n",
    "                    if line.strip().startswith(\"#\"):\n",
    "                        comments.append(line.strip().lstrip(\"# \").strip())\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing notebook {filepath}: {e}\")\n",
    "    return comments\n",
    "\n",
    "# Comprehensive extraction (docstrings, comments, full code)\n",
    "def extract_all(directory, ignore_dirs=None):\n",
    "    if ignore_dirs is None:\n",
    "        ignore_dirs = [\"data\"]\n",
    "\n",
    "    extracted_texts = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        # Skip any ignored directories\n",
    "        dirs[:] = [d for d in dirs if d not in ignore_dirs]\n",
    "\n",
    "        for file in files:\n",
    "            filepath = os.path.join(root, file)\n",
    "            try:\n",
    "                with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "                    content = f.read()\n",
    "\n",
    "                if file.endswith(\".py\"):\n",
    "                    docstrings = extract_docstrings(content)\n",
    "                    comments = extract_inline_comments(content)\n",
    "                    combined_content = f\"\"\"\n",
    "Filename: {file}\n",
    "\n",
    "Docstrings:\n",
    "{docstrings if docstrings else 'None'}\n",
    "\n",
    "Comments:\n",
    "{comments if comments else 'None'}\n",
    "\n",
    "Full Code:\n",
    "{content}\n",
    "                    \"\"\"\n",
    "                    extracted_texts.append(combined_content)\n",
    "\n",
    "                elif file.endswith(\".ipynb\"):\n",
    "                    comments = extract_notebook_comments(filepath)\n",
    "                    combined_content = f\"\"\"\n",
    "Notebook: {file}\n",
    "\n",
    "Comments:\n",
    "{comments if comments else 'None'}\n",
    "\n",
    "Full Notebook JSON Content:\n",
    "{content}\n",
    "                    \"\"\"\n",
    "                    extracted_texts.append(combined_content)\n",
    "\n",
    "                elif file.endswith(\".md\"):\n",
    "                    combined_content = f\"\"\"\n",
    "Markdown File: {file}\n",
    "\n",
    "Full Content:\n",
    "{content}\n",
    "                    \"\"\"\n",
    "                    extracted_texts.append(combined_content)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {filepath}: {e}\")\n",
    "\n",
    "    return extracted_texts\n",
    "\n",
    "# Run the comprehensive extraction\n",
    "structured_content = extract_all(CODEBASE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "999898ae-155e-4ef6-94b5-3c9684838b88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\nFilename: lsh.py\\n\\nDocstrings:\\nNone\\n\\nComments:\\n[\\'A label for this set\\', \\'Add to unionfind structure\\', \\'Get signature\\', \\'Union labels with same LSH key in same band\\', \\'Structure to be stored in the ConstrainedCluster.hashmaps band/hash cell\\', \\'cluster lists.\\', \\'Note that self.hashmaps, although having the same structure as in the\\', \\'parent class, is used quite differently here: each band/hash cell now\\', \\'corresponds to a list of lists (instead of a single list). Each list\\', \\'contains at least one LabelSetObj instance, and will possibly grow\\', \\'when hash collisions occur. However, to be fused within a certain\\', \\'list, an item must be similar enough to its first item (i.e. the\\', \\'constraint must be satisfied). If no list is found with an item to\\', \\'satisfy the constraint, a new list with the element is simply appended\\', \\'to the band/hash cell.\\', \\'A label for this set\\', \\'if obj is not defined, s is used\\', \\'Add to unionfind structure\\', \\'Get signature\\', \\'Union labels with same LSH key in same band that satisfy constraint\\', \\'apply the constraint function to compare the current element\\', \\'to every first element of every candidate clusters\\', \\'retain the best (if it exists) of those over the threshold\\', \\'the candidate pair is now clustered\\', \\'no clustering is performed\\', \\'Note that self.hashmaps, although having the same structure as in the\\', \\'parent class, is used quite differently here: each band/hash cell now\\', \\'corresponds to a list of lists (instead of a single list). Each list\\', \\'contains at least one LabelSetObj instance, and will possibly grow\\', \\'when hash collisions occur. However, to be fused within a certain\\', \\'list, an item must be similar enough to its first item (i.e. the\\', \\'constraint must be satisfied). If no list is found with an item to\\', \\'satisfy the constraint, a new list with the element is simply appended\\', \\'to the band/hash cell.\\', \\'A label for this set\\', \\'Union labels with same LSH key in same band that satisfy constraint\\', \\'apply the constraint function to compare the current element\\', \\'to every first element of every candidate clusters\\', \\'retain the best (if it exists) of those over the threshold\\', \\'the candidate pair is now clustered\\', \\'no clustering is performed\\']\\n\\nFull Code:\\n\"\"\"\\nlsh.py\\n\\nAlgorithms based on \\'Mining of Massive Datasets\\'\\n\"\"\"\\nfrom unionfind import UnionFind\\nfrom collections import defaultdict\\nfrom collections import defaultdict, namedtuple\\nfrom copy import deepcopy\\nimport operator\\n\\n\\ndef shingle(s, k):\\n    \"\"\"Generate k-length shingles of string s\"\"\"\\n    k = min(len(s), k)\\n    for i in range(len(s) - k + 1):\\n        yield s[i:i+k]\\n\\ndef hshingle(s, k):\\n    \"\"\"Generate k-length shingles then hash\"\"\"\\n    for s in shingle(s, k):\\n        yield hash(s)\\n\\ndef jaccard_sim(X, Y):\\n    \"\"\"Jaccard similarity between two sets\"\"\"\\n    x = set(X)\\n    y = set(Y)\\n    return float(len(x & y)) / len(x | y)\\n\\ndef jaccard_dist(X, Y):\\n    \"\"\"Jaccard distance between two sets\"\"\"\\n    return 1 - jaccard_sim(X, Y)\\n\\n\\nclass Signature(object):\\n    \"\"\"Signature Base class.\"\"\"\\n\\n    def __init__(self, dim):\\n        self.dim = dim\\n        self.hashes = self.hash_functions()\\n\\n    def hash_functions(self):\\n        \"\"\"Returns dim different hash functions\"\"\"\\n        pass\\n\\n    def sign(self, object):\\n        \"\"\"Return the signature for object s\"\"\"\\n        pass\\n\\n\\nclass MinHashSignature(Signature):\\n    \"\"\"Creates signatures for sets/tuples using minhash.\"\"\"\\n\\n    def hash_functions(self):\\n        \"\"\"Return dim different hash functions\"\"\"\\n        def hash_factory(n):\\n            return lambda x: hash(\"salt\" + str(n) + str(x) + \"salt\")\\n        return [ hash_factory(_) for _ in range(self.dim) ]\\n\\n    def sign(self, s):\\n        \"\"\"Returns minhash signature for set s\"\"\"\\n        sig = [ float(\"inf\") ] * self.dim\\n        for hash_ix, hash_fn in enumerate(self.hashes):\\n            sig[hash_ix] = min(hash_fn(value) for value in s)\\n        return sig\\n\\n\\nclass LSH(object):\\n    \"\"\"Locality sensitive hashing.  Uses a banding approach to hash\\n    similar signatures to the same buckets.\"\"\"\\n    def __init__(self, length, threshold):\\n        self.length = length\\n        self.threshold = threshold\\n        self.bandwidth = self.get_bandwidth(length, threshold)\\n\\n    def hash(self, sig, band_idx=None):\\n        \"\"\"Generate hashvals for this signature\"\"\"\\n        for band in zip(*(iter(sig),) * self.bandwidth):\\n            yield hash(\"salt\" + str(band) + \"tlas\")\\n\\n    def get_bandwidth(self, n, t):\\n        \"\"\"Approximates the bandwidth (number of rows in each band)\\n        needed to get threshold.\\n\\n        Threshold t = (1/b) ** (1/r) where\\n        b = #bands\\n        r = #rows per band\\n        n = b * r = #elements in signature\\n        \"\"\"\\n\\n        best = n, 1\\n        minerr  = float(\"inf\")\\n        for r in range(1, n + 1):\\n            try:\\n                b = 1. / (t ** r)\\n            except:             # Divide by zero, your signature is huge\\n                return best\\n            err = abs(n - b * r)\\n            if err < minerr:\\n                best = r\\n                minerr = err\\n        return best\\n\\n    def get_threshold(self):\\n        r = self.bandwidth\\n        b = self.length / r\\n        return (1. / b) ** (1. / r)\\n\\n    def get_n_bands(self):\\n        return int(self.length / self.bandwidth)\\n\\n\\nclass Cluster(object):\\n    \"\"\"Clusters sets with Jaccard similarity above threshold with high\\n    probability.\\n\\n    Algorithm based on Rajaraman, \"Mining of Massive Datasets\":\\n    1. Generate set signature\\n    2. Use LSH to map similar signatures to same buckets\\n    3. Use UnionFind to merge buckets containing same values\\n    \"\"\"\\n    def __init__(self, width=10, threshold=0.5):\\n        self.width = width\\n        self.unionfind = UnionFind()\\n        self.signer = MinHashSignature(width)\\n        self.hasher = LSH(width, threshold)\\n        self.hashmaps = [defaultdict(list)\\n                         for _ in range(self.hasher.get_n_bands())]\\n\\n    def add_set(self, s, label=None):\\n        # A label for this set\\n        if not label:\\n            label = s\\n\\n        # Add to unionfind structure\\n        self.unionfind[label]\\n\\n        # Get signature\\n        sig = self.signer.sign(s)\\n\\n        # Union labels with same LSH key in same band\\n        for band_idx, hshval in enumerate(self.hasher.hash(sig)):\\n            self.hashmaps[band_idx][hshval].append(label)\\n            self.unionfind.union(label, self.hashmaps[band_idx][hshval][0])\\n\\n    def get_sets(self):\\n        return self.unionfind.sets()\\n\\n\\nclass ConstrainedCluster(Cluster):\\n\\n    \"\"\"To fight the problem of big clusters created by the aggregation of a\\n    large number of false positives (i.e. two items found to be a candidate\\n    pair, but that really shouldn\\'t belong to the same cluster), this class\\n    introduces an extra constraint which must be met for two items to be\\n    clustered. This mechanism imposes that we keep track of extra items, that\\n    are encapsulated in the LabelObj namedtuple. The constraint, by default, is\\n    that the Jaccard Similarity must be as high as the hasher threshold, which\\n    is defined with this anonymous function:\\n\\n    lambda lo1, lo2: jaccard_sim(lo1.obj, lo2.obj)\\n\\n    where the lo\\'s are object of type LabelObj. However, this could be easily\\n    redefined to a function possibly more useful in some context, like the\\n    Levenshtein Ratio for instance (or any other similarity function to be\\n    maximized):\\n\\n    lambda lo1, lo2: Levenshtein.ratio(lo1.obj, lo2.obj)\\n\\n    which will work, provided that an \"obj\" argument has been previously passed\\n    to add_set.  In this case \"obj\" is a string, but it could be of whatever\\n    type, as long as the \"contraint_fn\" function properly handles it.\\n    \"\"\"\\n\\n    # Structure to be stored in the ConstrainedCluster.hashmaps band/hash cell\\n    # cluster lists.\\n    LabelObj = namedtuple(\\'LabelObj\\', \\'label obj\\')\\n\\n    def __init__(self, width=10, threshold=0.5,\\n                 constraint_min=None,\\n                 constraint_fn=lambda lo1, lo2:\\n                                   jaccard_sim(lo1.obj, lo2.obj)):\\n        super(ConstrainedCluster, self).__init__(width, threshold)\\n        if constraint_min is None:\\n            self.constraint_min = threshold\\n        else:\\n            self.constraint_min = constraint_min\\n        self.constraint_fn = constraint_fn\\n        # Note that self.hashmaps, although having the same structure as in the\\n        # parent class, is used quite differently here: each band/hash cell now\\n        # corresponds to a list of lists (instead of a single list). Each list\\n        # contains at least one LabelSetObj instance, and will possibly grow\\n        # when hash collisions occur. However, to be fused within a certain\\n        # list, an item must be similar enough to its first item (i.e. the\\n        # constraint must be satisfied). If no list is found with an item to\\n        # satisfy the constraint, a new list with the element is simply appended\\n        # to the band/hash cell.\\n\\n    def add_set(self, s, label=None, obj=None):\\n        # A label for this set\\n        if not label:\\n            label = s\\n\\n        # if obj is not defined, s is used\\n        lo = ConstrainedCluster.LabelObj(label, obj if obj else s)\\n\\n        # Add to unionfind structure\\n        self.unionfind[label]\\n\\n        # Get signature\\n        sig = self.signer.sign(s)\\n\\n        # Union labels with same LSH key in same band that satisfy constraint\\n        for band_idx, hshval in enumerate(self.hasher.hash(sig)):\\n            # apply the constraint function to compare the current element\\n            # to every first element of every candidate clusters\\n            jsc = [(self.constraint_fn(lo, cluster[0]), cluster)\\n                   for cluster in self.hashmaps[band_idx][hshval]]\\n            # retain the best (if it exists) of those over the threshold\\n            jsc = sorted([(js, cluster) for js, cluster in jsc\\n                          if js >= self.constraint_min], reverse=True)\\n            if jsc:\\n                cluster = jsc[0][1]\\n                cluster.append(deepcopy(lo))\\n                # the candidate pair is now clustered\\n                self.unionfind.union(lo.label, cluster[0].label)\\n            else:\\n                # no clustering is performed\\n                self.hashmaps[band_idx][hshval].append([deepcopy(lo)])\\n\\n\\nclass SemiParallellizableConstrainedCluster(Cluster):\\n\\n    \"\"\"This is a semi-parallel version of ConstrainedCluster, to be used with\\n    multiprocessing; explanations and documentation soon to come..\\n    \"\"\"\\n\\n    def __init__(self, width=10, threshold=0.5,\\n                 constraint_min=None,\\n                 constraint_fn=lambda lo1, lo2:\\n                                   jaccard_sim(lo1.obj, lo2.obj),\\n                 sigmaps_to_merge=None):\\n        super(SemiParallellizableConstrainedCluster, self).__init__(width, threshold)\\n        if constraint_min is None:\\n            self.constraint_min = threshold\\n        else:\\n            self.constraint_min = constraint_min\\n        self.constraint_fn = constraint_fn\\n        # Note that self.hashmaps, although having the same structure as in the\\n        # parent class, is used quite differently here: each band/hash cell now\\n        # corresponds to a list of lists (instead of a single list). Each list\\n        # contains at least one LabelSetObj instance, and will possibly grow\\n        # when hash collisions occur. However, to be fused within a certain\\n        # list, an item must be similar enough to its first item (i.e. the\\n        # constraint must be satisfied). If no list is found with an item to\\n        # satisfy the constraint, a new list with the element is simply appended\\n        # to the band/hash cell.\\n        if sigmaps_to_merge is None:\\n            self.sigmap = {}\\n        else:\\n            self.sigmap = dict(reduce(operator.__add__,\\n                                      [sm.items() for sm in sigmaps_to_merge]))\\n\\n    def sign(self, s, label=None, obj=None):\\n        # A label for this set\\n        if not label:\\n            label = s\\n        self.sigmap[label] = (self.signer.sign(s) if s else None,\\n                              obj if obj else s)\\n\\n    def find_clusters(self):\\n        for label, (sig, obj) in self.sigmap.iteritems():\\n            self.unionfind[label]\\n            if sig is None: continue\\n            lo = ConstrainedCluster.LabelObj(label, obj)\\n\\n            # Union labels with same LSH key in same band that satisfy constraint\\n            for band_idx, hshval in enumerate(self.hasher.hash(sig)):\\n                # apply the constraint function to compare the current element\\n                # to every first element of every candidate clusters\\n                jsc = [(self.constraint_fn(lo, cluster[0]), cluster)\\n                       for cluster in self.hashmaps[band_idx][hshval]]\\n                # retain the best (if it exists) of those over the threshold\\n                jsc = sorted([(js, cluster) for js, cluster in jsc\\n                              if js >= self.constraint_min], reverse=True)\\n                if jsc:\\n                    cluster = jsc[0][1]\\n                    cluster.append(deepcopy(lo))\\n                    # the candidate pair is now clustered\\n                    self.unionfind.union(lo.label, cluster[0].label)\\n                else:\\n                    # no clustering is performed\\n                    self.hashmaps[band_idx][hshval].append([deepcopy(lo)])\\n\\n\\nif __name__ == \\'__main__\\':\\n\\n    n = 2\\n    sa = set(shingle(\"1234abcdef\", n))\\n    sb = set(shingle(\"4321abcdef\", n))\\n\\n    print \\'Jaccard Sim:\\', jaccard_sim(sa, sb)\\n\\n    cluster = Cluster()\\n    cluster.add_set(sa, \\'a\\')\\n    cluster.add_set(sb, \\'b\\')\\n    print \\'Cluster:\\', cluster.get_sets() # [[\\'a\\', \\'b\\']]\\n\\n    cluster = ConstrainedCluster()\\n    cluster.add_set(sa, \\'a\\')\\n    cluster.add_set(sb, \\'b\\')\\n    print \\'ConstrainedCluster:\\', cluster.get_sets() # [[\\'a\\'], [\\'b\\']]\\n\\n                    ',\n",
       " '\\nMarkdown File: README.md\\n\\nFull Content:\\n## LSHHDC : Locality-Sensitive Hashing based High Dimensional  Clustering  \\n\\n## Locality-sensitive hashing  \\nUnlike cryptographic hashing where the goal is to map objects to\\nnumbers with a low collision rate and high randomness, the goal of LSH\\nis to map similar elements to similar keys with high probability.\\n\\nAn obvious use of this technique is clustering.  From Rajamaran,\\n\"Mining of Massive Datasets\":\\n\\n> A family F of functions is said to be (d1, d2, p1, p2)-sensitive if\\n> for every f in F:  \\n> 1. If d(x,y) ≤ d1, then the probability that f(x) = f(y) is at least p1.  \\n> 2. If d(x,y) ≥ d2, then the probability that f(x) = f(y) is at most p2.  \\n\\n\\n\\n\\n                    ',\n",
       " '\\nFilename: unionfind.py\\n\\nDocstrings:\\n[\"Module \\'Module\\': UnionFind.py\\\\n\\\\nSource: http://www.ics.uci.edu/~eppstein/PADS/UnionFind.py\\\\n\\\\nUnion-find data structure. Based on Josiah Carlson\\'s code,\\\\nhttp://aspn.activestate.com/ASPN/Cookbook/Python/Recipe/215912\\\\nwith significant additional changes by D. Eppstein.\", \"ClassDef \\'UnionFind\\': Union-find data structure.\\\\n\\\\nEach unionFind instance X maintains a family of disjoint sets of\\\\nhashable objects, supporting the following two methods:\\\\n\\\\n- X[item] returns a name for the set containing the given item.\\\\n  Each set is named by an arbitrarily-chosen one of its members; as\\\\n  long as the set remains unchanged it will keep the same name. If\\\\n  the item is not yet part of a set in X, a new singleton set is\\\\n  created for it.\\\\n\\\\n- X.union(item1, item2, ...) merges the sets containing each item\\\\n  into a single larger set.  If any item is not yet part of a set\\\\n  in X, it is added to X as one of the members of the merged set.\", \"FunctionDef \\'__init__\\': Create a new empty union-find structure.\", \"FunctionDef \\'__getitem__\\': Find and return the name of the set containing the object.\", \"FunctionDef \\'__iter__\\': Iterate through all items ever found or unioned by this structure.\", \"FunctionDef \\'union\\': Find the sets containing the objects and merge them all.\", \"FunctionDef \\'sets\\': Return a list of each disjoint set\"]\\n\\nComments:\\n[\\'check for previously unknown object\\', \\'find path of objects leading to the root\\', \\'compress the path and return\\', \\'test\\']\\n\\nFull Code:\\n\"\"\"\\nUnionFind.py\\n\\nSource: http://www.ics.uci.edu/~eppstein/PADS/UnionFind.py\\n\\nUnion-find data structure. Based on Josiah Carlson\\'s code,\\nhttp://aspn.activestate.com/ASPN/Cookbook/Python/Recipe/215912\\nwith significant additional changes by D. Eppstein.\\n\"\"\"\\n\\nfrom collections import defaultdict\\n\\nclass UnionFind:\\n    \"\"\"Union-find data structure.\\n\\n    Each unionFind instance X maintains a family of disjoint sets of\\n    hashable objects, supporting the following two methods:\\n\\n    - X[item] returns a name for the set containing the given item.\\n      Each set is named by an arbitrarily-chosen one of its members; as\\n      long as the set remains unchanged it will keep the same name. If\\n      the item is not yet part of a set in X, a new singleton set is\\n      created for it.\\n\\n    - X.union(item1, item2, ...) merges the sets containing each item\\n      into a single larger set.  If any item is not yet part of a set\\n      in X, it is added to X as one of the members of the merged set.\\n    \"\"\"\\n\\n    def __init__(self):\\n        \"\"\"Create a new empty union-find structure.\"\"\"\\n        self.weights = {}\\n        self.parents = {}\\n\\n    def __getitem__(self, object):\\n        \"\"\"Find and return the name of the set containing the object.\"\"\"\\n        # check for previously unknown object\\n        if object not in self.parents:\\n            self.parents[object] = object\\n            self.weights[object] = 1\\n            return object\\n\\n        # find path of objects leading to the root\\n        path = [object]\\n        root = self.parents[object]\\n        while root != path[-1]:\\n            path.append(root)\\n            root = self.parents[root]\\n\\n        # compress the path and return\\n        for ancestor in path:\\n            self.parents[ancestor] = root\\n        return root\\n\\n    def __iter__(self):\\n        \"\"\"Iterate through all items ever found or unioned by this structure.\"\"\"\\n        return iter(self.parents)\\n\\n    def union(self, *objects):\\n        \"\"\"Find the sets containing the objects and merge them all.\"\"\"\\n        roots = [self[x] for x in objects]\\n        heaviest = max([(self.weights[r],r) for r in roots])[1]\\n        for r in roots:\\n            if r != heaviest:\\n                self.weights[heaviest] += self.weights[r]\\n                self.parents[r] = heaviest\\n\\n    def sets(self):\\n        \"\"\"Return a list of each disjoint set\"\"\"\\n        ret = defaultdict(list)\\n        for k, _ in self.parents.iteritems():\\n            ret[self[k]].append(k)\\n        return ret.values()\\n\\n    \\nif __name__ == \\'__main__\\':\\n\\n    # test\\n    uf = UnionFind()\\n    uf.union(0, 1)\\n    uf.union(2, 3)\\n    uf.union(3, 0)\\n    assert uf.sets() == [[0, 1, 2, 3]]\\n\\n                    ',\n",
       " '\\nFilename: __init__.py\\n\\nDocstrings:\\nNone\\n\\nComments:\\nNone\\n\\nFull Code:\\n\\n                    ',\n",
       " '\\nFilename: test_cluster.py\\n\\nDocstrings:\\nNone\\n\\nComments:\\n[\\'Get some sets and their similarities\\', \\'Find the threshold at which they cluster together\\']\\n\\nFull Code:\\nfrom utils import *\\nfrom ..lsh import Cluster, jaccard_sim\\n\\ndef test_same_set():\\n    \"\"\"A set should be clustered with itself\"\"\"\\n    s = randset()\\n    cluster = Cluster()\\n    cluster.add_set(s)\\n    cluster.add_set(s)\\n    assert len(cluster.get_sets()) == 1\\n\\ndef test_similar_sets():\\n    \"\"\"Two similar sets should be clustered\"\"\"\\n    cluster = Cluster()\\n    cluster.add_set(\"abcdefg\")\\n    cluster.add_set(\"abcdefghi\")\\n    assert len(cluster.get_sets()) == 1\\n\\ndef test_dissimilar_sets():\\n    \"\"\"Two non-similar sets should not be clustered\"\"\"\\n    cluster = Cluster()\\n    cluster.add_set(\"12345abcdef\")\\n    cluster.add_set(\"1234567890z\")\\n    print cluster.get_sets()\\n    assert len(cluster.get_sets()) == 2\\n\\ndef test_cluster_threshold():\\n    \"\"\"Expected error for threshold to similarity should be reasonable\"\"\"\\n    n_tests = 50\\n    dim = 15\\n    expected_error = 0.20\\n    \\n    tot_err = 0\\n    for test in range(n_tests):\\n        # Get some sets and their similarities\\n        sets = (randset(), randset())\\n        jsim = jaccard_sim(*sets)\\n        \\n        # Find the threshold at which they cluster together\\n        for threshold in range(1, 100, 5):\\n            threshold = float(threshold) / 100\\n            cluster = Cluster(dim, threshold)\\n            cluster.add_set(sets[0])\\n            cluster.add_set(sets[1])\\n            if len(cluster.get_sets()) == 2:\\n                tot_err += abs(jsim - threshold)\\n                break\\n    avg_err = float(tot_err) / n_tests\\n    assert avg_err <= expected_error\\n\\n                    ',\n",
       " '\\nFilename: test_sig.py\\n\\nDocstrings:\\n[\"FunctionDef \\'test_signature_length\\': Signatures should have correct dimension\", \"FunctionDef \\'test_consistent_signature\\': Signatures should be consistent\", \"FunctionDef \\'test_signature_similarity\\': The probability that two sets\\' signatures match at some index\\\\nare equal is equal to the Jaccard similarity between the two\"]\\n\\nComments:\\n[\\'Create random sets and their signatures\\', \\'Calculate true jaccard similarity, and sim of signatures\\', \\'Accumulate error\\', \\'Over n_tests large, we should be within upper bound of expected error.\\']\\n\\nFull Code:\\nimport itertools\\n\\nfrom math import sqrt\\n\\nfrom utils import *\\nfrom ..lsh import MinHashSignature, jaccard_sim\\n\\ndef test_signature_length():\\n    \"\"\"Signatures should have correct dimension\"\"\"\\n    dim = 100\\n    mh = MinHashSignature(dim)\\n    assert dim == len(mh.sign(randset()))\\n\\ndef test_consistent_signature():\\n    \"\"\"Signatures should be consistent\"\"\"\\n    mh = MinHashSignature(100)\\n    s = randset()\\n    assert mh.sign(s) == mh.sign(s)\\n\\ndef test_signature_similarity():\\n    \"\"\"The probability that two sets\\' signatures match at some index\\n    are equal is equal to the Jaccard similarity between the two\"\"\"\\n    dim = 100\\n    n_tests = 100\\n    expected_error = 1 / sqrt(dim) # Expected error is O(1/sqrt(dim))\\n    mh = MinHashSignature(dim)\\n    err = 0.0\\n\\n    for test in range(n_tests):\\n        # Create random sets and their signatures\\n        sets = (randset(), randset())\\n        sigs = map(mh.sign, sets)\\n\\n        # Calculate true jaccard similarity, and sim of signatures\\n        jsim = jaccard_sim(*sets)\\n        ssim = sigsim(*sigs, dim=dim)\\n\\n        # Accumulate error\\n        err += abs(jsim - ssim)\\n\\n    # Over n_tests large, we should be within upper bound of expected error.\\n    avg_err = err / n_tests\\n    assert expected_error >= avg_err, \"Accuracy test failed. (avg error: %f)\" % avg_err\\n\\n                    ',\n",
       " '\\nFilename: utils.py\\n\\nDocstrings:\\n[\"Module \\'Module\\': utils.py\\\\n\\\\nTesting utilities\", \"FunctionDef \\'randset\\': Return a random set.  These values of n and k have wide-ranging\\\\nsimilarities between pairs.\", \"FunctionDef \\'sigsim\\': Return the similarity of the two signatures\"]\\n\\nComments:\\nNone\\n\\nFull Code:\\n\"\"\"\\nutils.py\\n\\nTesting utilities\\n\"\"\"\\nimport random\\nimport operator\\n\\ndef randset():\\n    \"\"\"Return a random set.  These values of n and k have wide-ranging\\n    similarities between pairs.\\n    \"\"\"\\n    n = random.choice(range(5, 20))\\n    k = 10\\n    return tuple(set( random.choice(range(k)) for _ in range(n) ))\\n\\ndef sigsim(X, Y, dim):\\n    \"\"\"Return the similarity of the two signatures\"\"\"\\n    return sum(map(operator.eq, X, Y)) / float(dim)\\n\\n                    ',\n",
       " '\\nFilename: __init__.py\\n\\nDocstrings:\\nNone\\n\\nComments:\\nNone\\n\\nFull Code:\\n\\n                    ']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "structured_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5aa61aa0-5561-486f-b859-7984f868df8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:nano-graphrag:[New Docs] inserting 7 docs\n",
      "INFO:nano-graphrag:[New Chunks] inserting 9 chunks\n",
      "INFO:nano-graphrag:[Entity Extraction]...\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.870000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 5.324000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 2.562000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 2.580000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 4.392000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⠙ Processed 1 chunks, 0 entities(duplicated), 0 relations(duplicated)\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 2.118000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 4.432000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⠹ Processed 2 chunks, 0 entities(duplicated), 0 relations(duplicated)\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 3.618000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 4.614000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 4.584000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⠸ Processed 3 chunks, 3 entities(duplicated), 2 relations(duplicated)\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 3.050000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⠼ Processed 4 chunks, 8 entities(duplicated), 4 relations(duplicated)\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 4.696000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 4.222000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 4.310000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 5.578000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 5.808000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⠴ Processed 5 chunks, 17 entities(duplicated), 5 relations(duplicated)\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.009000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.132000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 7.118000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 7.064000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 6.856000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⠦ Processed 6 chunks, 17 entities(duplicated), 5 relations(duplicated)\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⠧ Processed 7 chunks, 27 entities(duplicated), 9 relations(duplicated)\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.838000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.812000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 7.530000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⠇ Processed 8 chunks, 37 entities(duplicated), 15 relations(duplicated)\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⠏ Processed 9 chunks, 48 entities(duplicated), 20 relations(duplicated)\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:nano-graphrag:Inserting 40 vectors to entities\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:nano-graphrag:[Community Report]...\n",
      "INFO:nano-graphrag:Each level has communities: {0: 2}\n",
      "INFO:nano-graphrag:Generating by levels: [0]\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⠙ Processed 1 communities\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⠹ Processed 2 communities\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:nano-graphrag:Writing graph with 42 nodes, 20 edges\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Insert into GraphRAG\n",
    "await graph_rag.ainsert(structured_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "200b2aea-6a51-49b1-a071-3e89a1e8568f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:nano-graphrag:Revtrieved 2 communities\n",
      "INFO:nano-graphrag:Grouping to 1 groups for global search\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🟢 Global Query Response:\n",
      " ### Key Principles and Methodologies\n",
      "\n",
      "The repository implements several key principles and methodologies to facilitate effective high-dimensional data clustering. The cornerstone of these methods is the use of **Jaccard Similarity and Distance**, which are essential for evaluating similarity and dissimilarity between sets. These metrics provide a statistical basis for identifying likeness and contrast among data sets, which is crucial for clustering operations in high-dimensional spaces.\n",
      "\n",
      "### MinHashSignature and Its Integration\n",
      "\n",
      "A significant methodological approach employed is the **MinHashSignature**. This technique is designed to generate minhash signatures for datasets, enabling comparisons based on Jaccard Similarity. By condensing the high-dimensional data into a representative hash signature, this method supports efficient clustering by focusing on the core attributes of the data.\n",
      "\n",
      "Moreover, MinHashSignature is effectively integrated with **Locality-Sensitive Hashing (LSH)**. This integration allows data to be partitioned into buckets based on their similarity, thus facilitating the grouping of similar data points. This is particularly valuable in high-dimensional spaces, where traditional clustering methods may struggle.\n",
      "\n",
      "### Data Preparation through Shingle and Hshingle\n",
      "\n",
      "A critical preparatory step in the methodology is data preprocessing using the **Shingle and Hshingle** functions. The Shingle function breaks down strings into elements, while Hshingle hashes these elements for efficient computation. This preprocessing ensures that the data is in a suitable format for subsequent similarity testing, forming the groundwork for accurate clustering in high-dimensional data analysis.\n",
      "\n",
      "Through these methodologies, the repository provides a robust and efficient framework for dealing with the complexities of high-dimensional data clustering, leveraging statistical measures, advanced hashing techniques, and effective data preprocessing strategies.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:nano-graphrag:Using 20 entites, 1 communities, 14 relations, 4 text units\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔵 Local Query Response:\n",
      " The **lsh.py** module implements the Locality-Sensitive Hashing (LSH) algorithm, which is designed to group similar data points into clusters. This technique is particularly useful for high-dimensional data where traditional hashing techniques may not efficiently handle similarity-based data transformations. LSH utilizes a probabilistic approach, mapping similar elements to similar hash keys while maintaining a high probability. The cornerstone of this method is the concept of creating 'bands' of hash values, which enables the system to efficiently identify clusters of similar data based on predefined similarity thresholds. This module takes a significant cue from the concept of LSH as discussed in the context of \"Mining of Massive Datasets,\" by Rajamaran, describing how similar objects are likely to share similar keys through precise function family definitions.\n",
      "\n",
      "### Core Functions in lsh.py:\n",
      "\n",
      "1. **Shingle and Hshingle Functions**: \n",
      "   - The `shingle(s, k)` function generates k-length shingles from a given string `s`, essentially breaking the string into smaller overlapping substrings, which are vital for initiating the hashing process.\n",
      "   - The `hshingle(s, k)` function extends this by first generating shingles and then applying a hash function to each, thereby transforming the substrings into hash values that are more efficiently handled by the algorithm.\n",
      "\n",
      "2. **Jaccard Similarity and Distance**:\n",
      "   - The `jaccard_sim(X, Y)` function calculates the Jaccard Similarity between two sets. This involves computing the size of the intersection divided by the size of the union of the sets, providing a normalized measure of set similarity.\n",
      "   - The `jaccard_dist(X, Y)` function complements this process by providing the Jaccard Distance, which is simply calculated as one minus the Jaccard Similarity, indicating dissimilarity.\n",
      "\n",
      "3. **Signature and MinHashSignature**:\n",
      "   - The module defines a `Signature` base class with functions to generate hash functions and signatures. The `MinHashSignature` class extends this, specifically implementing the MinHashing technique to generate compact signatures for sets or tuples based on minimized hash values across a collection of hash functions. These minhash signatures are fundamental in grouping similar datasets.\n",
      "\n",
      "4. **Locality-Sensitive Hashing (LSH)**:\n",
      "   - The `LSH` class uses a banding approach to generate and manage hash buckets. It involves key functions such as `hash()`, which effectively divides hash signatures into multiple bands and generates hash values for each band, ensuring that similar signatures are mapped to the same hash buckets. By adjusting the `get_bandwidth()` method, it determines the optimal bandwidth to achieve desired similarity thresholds, balancing between the number of bands and the rows within each band.\n",
      "\n",
      "The LSH implementation in `lsh.py` provides a robust framework for clustering high-dimensional data by efficiently managing and computing similarity through hashing. The module stands as a testament to the versatility and power of LSH in solving complex clustering problems, primarily driven by its intelligently constructed methods for similarity measurement and data organization.\n"
     ]
    }
   ],
   "source": [
    "# Example Queries:\n",
    "\n",
    "# Global query example (about the whole codebase):\n",
    "global_response = await graph_rag.aquery(\"What are the key principles and methodologies implemented in this repository for high-dimensional data clustering?\")\n",
    "print(\"\\n🟢 Global Query Response:\\n\", global_response)\n",
    "\n",
    "# Local query example (about a specific script or function):\n",
    "local_response = await graph_rag.aquery(\n",
    "    \"How does the lsh.py module implement the LSH algorithm, and what are the specific functions responsible for hashing and clustering operations?\",\n",
    "    param=QueryParam(mode=\"local\")\n",
    ")\n",
    "print(\"\\n🔵 Local Query Response:\\n\", local_response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
